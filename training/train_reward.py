# -*- coding: utf-8 -*-
"""Train xlm-r reward model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eRRlMB12VPOVmQjMCD5Qg1YoIa-vpgxp
"""

pip install datasets transformers evaluate trl 

from google.colab import drive
drive.mount('/content/drive')
checkpoint_dir = '/content/drive/MyDrive/model_checkpoints'

from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments
from trl import RewardTrainer, RewardConfig
from datasets import load_dataset
import re
import string
import torch

def normalize_text(text):
    text = ' '.join(text.split())
    text = re.sub(r'https?://\S+', '', text)
    text = re.sub(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', '', text)
    text = re.sub(r'[^\x00-\x7F]+', '', text)
    return text

train_ds = load_dataset("Anthropic/hh-rlhf", split='train')
val_ds = load_dataset("Anthropic/hh-rlhf", split='test[0:2000]')
test_ds = load_dataset("Anthropic/hh-rlhf", split='test[2000:]')

normalize_text(train_ds[0]['rejected'])

tokenizer = AutoTokenizer.from_pretrained("gpt2", pad_token = '[PAD]')
model = AutoModelForSequenceClassification.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

def custom_tokenize_and_pad(sentence, max_length=1024):
    tokens = tokenizer.tokenize(normalize_text(sentence))
    if len(tokens) > max_length:
        tokens = tokens[-max_length:]  
    padding_length = max_length - len(tokens)
    if padding_length > 0:
        tokens = [tokenizer.pad_token] * padding_length + tokens 
    token_ids = tokenizer.convert_tokens_to_ids(tokens)

    attention_mask = [1] * (max_length - padding_length) 
    attention_mask += [0] * padding_length  
    input_ids = torch.tensor(token_ids)
    attention_mask_tensor = torch.tensor(attention_mask)

    assert len(input_ids) == max_length
    assert len(attention_mask_tensor) == max_length
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask_tensor
    }

def tokenize_function(examples):
    chosen_tokens = custom_tokenize_and_pad(examples['chosen'])
    rejected_tokens = custom_tokenize_and_pad(examples['rejected'])
    return {
        'input_ids_chosen': chosen_tokens['input_ids'],
        'attention_mask_chosen': chosen_tokens['attention_mask'],
        'input_ids_rejected': rejected_tokens['input_ids'],
        'attention_mask_rejected': rejected_tokens['attention_mask']
    }

tokenized_train = train_ds.map(tokenize_function)
# tokenized_test = test_ds.map(tokenize_function)
tokenized_val = val_ds.map(tokenize_function)

tokenized_train.remove_columns(['chosen', 'rejected'])
# tokenized_test.remove_columns(['chosen', 'rejected'])
tokenized_val.remove_columns(['chosen', 'rejected'])

# Define TrainingArguments
training_args = RewardConfig(
    output_dir=f'{checkpoint_dir}/gpt2',         
    evaluation_strategy="epoch",      
    learning_rate=2e-5,               
    per_device_train_batch_size=16,    
    per_device_eval_batch_size=16,   
    num_train_epochs=10,            
    weight_decay=0.01,               
    logging_dir='./logs',             
    logging_steps=50,                
    save_steps=1000,                  
    save_total_limit=2,             
    metric_for_best_model="accuracy",
    fp16=True,                        
    gradient_accumulation_steps=2,    
    center_rewards_coefficient=0.01,
    max_length=1024,
)

model.config.pad_token_id = tokenizer.pad_token_id

trainer = RewardTrainer(
    args=training_args,
    model=model,
    processing_class=tokenizer,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val
)

tokenizer.pad_token

trainer.train()

