# -*- coding: utf-8 -*-
"""Train llm2vec

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zo3CUsXFp-FdYrcyZfTTMoeoUo6mCYNS
"""

!pip install LLM2Vec datasets

!pip install huggingface_hub
!huggingface-cli login
from huggingface_hub import notebook_login
notebook_login()

from tqdm import tqdm

from datasets import load_dataset

dataset = "Anthropic/hh-rlhf"
train_unpr = load_dataset(dataset, split='train')
val_unpr = load_dataset(dataset, split='test[0:2000]')
test_unpr = load_dataset(dataset, split='test[2000:]')
train = []
test = []
for elem in tqdm(train_unpr):
  chosen = elem['chosen']
  rejected = elem['rejected']
  chosen_example = {'text': chosen, 'labels': [1]}
  rejected_example = {'text': rejected, 'labels': [0]}
  # Add the new examples to the transformed dataset
  train.extend([chosen_example, rejected_example])

for elem in tqdm(test_unpr):
  chosen = elem['chosen']
  rejected = elem['rejected']
  chosen_example = {'text': chosen, 'labels': [1]}
  rejected_example = {'text': rejected, 'labels': [0]}
  # Add the new examples to the transformed dataset
  test.extend([chosen_example, rejected_example])

from sklearn.metrics import accuracy_score, f1_score
from sklearn.linear_model import LogisticRegression
import datasets
import numpy as np

import torch
from llm2vec import LLM2Vec

from transformers import AutoTokenizer, AutoModel, AutoConfig

dataset = "Anthropic/hh-rlhf"
instruction = "Classify a given conversation as safe or unsafe."

dataset = datasets.load_dataset(dataset)

max_iter = 100
batch_size = 8

scores = {}
clf = LogisticRegression(
    random_state=42,
    n_jobs=1,
    max_iter=max_iter,
    verbose=0,
)

config = AutoConfig.from_pretrained(
    "McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp", trust_remote_code=True
)

print("Loading model...")
model = AutoModel.from_pretrained(
    "McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp",
    trust_remote_code=True,
    config=config,
    torch_dtype=torch.bfloat16,
    device_map="cuda" if torch.cuda.is_available() else "cpu",
)

tokenizer = AutoTokenizer.from_pretrained(
    "McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp"
)

l2v = LLM2Vec(model, tokenizer, pooling_mode="mean", max_length=512)

sentences_train = [elem["text"] for elem in train]
y_train = [elem["labels"] for elem in train]
sentences_test = [elem["text"] for elem in test]
y_test = [elem["labels"] for elem in test]

def append_instruction(instruction, sentences):
    new_sentences = []
    for s in sentences:
        new_sentences.append([instruction, s, 0])
    return new_sentences

sample_batch = sentences_train[0:8]

batch_enc = l2v.encode(sample_batch, batch_size=batch_size, convert_to_tensor=True, device='cuda')

print(f"Encoding {len(sentences_train)} training sentences...")
sentences_train = append_instruction(instruction, sentences_train)
X_train = np.asarray(l2v.encode(sentences_train, batch_size=batch_size))

print(f"Encoding {len(sentences_test)} test sentences...")
sentences_test = append_instruction(instruction, sentences_test)
X_test = np.asarray(l2v.encode(sentences_test, batch_size=batch_size))

print("Fitting logistic regression classifier...")
clf.fit(X_train, y_train)
print("Evaluating...")
y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
scores["accuracy"] = accuracy
f1 = f1_score(y_test, y_pred, average="macro")
scores["f1"] = f1

print(scores)
# {'accuracy': 0.891044776119403, 'f1': 0.8283106625713033}

